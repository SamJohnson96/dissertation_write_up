%!TeX spellcheck = en-US,en-DE
\documentclass[a4paper,11pt]{report}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% University of Sussex thesis template
%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Modification History
%
% Based on usthesis.cls by Jonathon Read
% http://www.cogs.susx.ac.uk/users/jlr24/latex.html
% Modified by Anthony Smith, Feb 2007
% Incorporated into single thesis.tex file, Anthony Smith, 30 June 2008
% Minor alterations to page numbering, AJS, 25 July 2008
% New alternative hyperref options for print version, AJS, 11 Sep 2008
% "DRAFT" on header, AJS, 12 Sep 2008
%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% LINE SPACING
\newcommand{\linespacing}{1.5}
\renewcommand{\baselinestretch}{\linespacing}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% BIBLIOGRAPHY STYLE
\usepackage{natbib}
% \bibliographystyle{plain} for [1], [2] etc.
\bibliographystyle{apalike}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% OTHER FORMATTING/LAYOUT DECLARATIONS
% Graphics
\usepackage{graphicx,color}
\usepackage{csquotes}
\usepackage{epstopdf}
\usepackage[british]{babel}
% The left-hand-side should be 40mm.  The top and bottom margins should be
% 25mm deep.  The right hand margin should be 20mm.
\usepackage[a4paper,top=2.5cm,bottom=2.5cm,left=4cm,right=2cm,headsep=10pt]{geometry}
\flushbottom
% Pages should be numbered consecutively thorugh the main text.  Page numbers
% should be located centrally at the top of the page.
\usepackage{fancyhdr}
\fancypagestyle{plain}{
	\fancyhf{}
	% Add "DRAFT: <today's date>" to header (comment out the following to remove)
	%\lhead{\textit{DRAFT: \today}}
	%
	\chead{\thepage}
	\renewcommand{\headrulewidth}{0pt}
}
\pagestyle{plain}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% ANY OTHER DECLARATIONS HERE:

%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% HYPERREF
\usepackage[colorlinks,pagebackref,pdfusetitle,urlcolor=blue,citecolor=blue,linkcolor=blue,bookmarksnumbered,plainpages=false]{hyperref}
% For print version, use this instead:
%\usepackage[pdfusetitle,bookmarksnumbered,plainpages=false]{hyperref}
%\usepackage{backref}
%\renewcommand{\backrefpagesname}{Cited on}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% BEGIN DOCUMENT
\begin{document}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% PREAMBLE: roman page numbering i, ii, iii, ...
\pagenumbering{roman}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% TITLE PAGE: The title page should give the following information:
%%	(i) the full title of the thesis and the sub-title if any;
%%	(ii) the full name of the author;
%%	(iii) the qualification aimed for;
%%	(iv) the name of the University of Sussex;
%%	(v) the month and year of submission.
\thispagestyle{empty}
\begin{flushright}
\includegraphics[width=6cm]{uslogo}
\end{flushright}
\vskip40mm
\begin{center}
% TITLE
\huge\textbf{Predicting share prices using natural language engineering and machine learning techniques.}
\vskip2mm
% SUBTITLE (optional)
% \LARGE\textit{How it all works}
\vskip5mm
% AUTHOR
\Large\textbf{Samuel Johnson}
\normalsize
\end{center}
\vfill
\begin{flushleft}
\large
% QUALIFICATION
University of Sussex	\\
% DATE OF SUBMISSION
May	 2018
\end{flushleft}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% DECLARATIONS
\chapter*{Declaration}
This report is submitted as part requirement for the degree of Computer Science at the University of Sussex. It is the product of my own labour except where indicated in the text. The report may be freely copied and distributed provided the source is acknowledged

% ADDITIONAL DECLARATIONS HERE (IF ANY)

\vskip5mm
Signature:
\vskip20mm
% AUTHOR
Samuel Johnson
%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% ACKNOWLEDGEMENTS
\chapter*{Acknowledgements}
\renewcommand{\baselinestretch}{\linespacing}
\small\normalsize
% ACKNOWLEDGEMENTS HERE:
Bill Keller\\
Dad\\
Mum\\
Robert\\
Emily\\
%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% SUMMARY PAGE
\thispagestyle{empty}
\newpage
\null\vskip10mm
\begin{center}
\large
\underline{UNIVERSITY OF SUSSEX}
\vskip20mm
% AUTHOR, QUALIFICATION
\textsc{Samuel Johnson}
\vskip20mm
% TITLE
\underline{\textsc{Predicting share prices using natural language engineering and machine learning techniques}}
\vskip0mm
% SUBTITLE (optional)
% \underline{\textsc{How it all works}}
\vskip20mm
\underline{\textsc{Summary}}
\vskip2mm
\end{center}
% Change line spacing
\renewcommand{\baselinestretch}{1.0}
\small\normalsize
% SUMMARY HERE (300 word limit for most subjects):

%%%%%%%%%%%%%%%%%%%%%%%%%%%%




%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% TABLE OF CONTENTS, LISTS OF TABLES & FIGURES
\newpage
\pdfbookmark[0]{Contents}{contents_bookmark}
\tableofcontents
\listoftables
\phantomsection
\addcontentsline{toc}{chapter}{List of Tables}
\listoffigures
\phantomsection
\addcontentsline{toc}{chapter}{List of Figures}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% MAIN THESIS TEXT: arabic page numbering 1, 2, 3, ...
\newpage
\pagenumbering{arabic}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%-----------------------------------------------------
% Chapter: Introduction
%-----------------------------------------------------
\chapter{Introduction}
\label{chap:intro}

Data mining and natural language processing are some of the fastest growing fields in computer science. This is a result of many factors, the most notable being the emergence of the world wide web.  Consequently, millions of interconnected documents can be viewed and shared by 3.2 billion people. This continuous stream of information provides one of the largest and diverse data sets to mine and harvest information from. Another factor is the investment that commercial companies are making to see how they can exploit large data sets to their advantage. An example of this is mining and extracting customer opinions about a given product found on the web. Using this information could lead to improvements in a company's product and also give a better understanding of their customer's needs.

One sector that is investing resources in data mining and natural language processing is the financial sector. Brokerage firms and investors are spending their time and money into harvesting a range of data. This is with the aim that the data will aid them in making better decisions when buying or selling shares.  The type of data collected can range from statistical financial data to textual information from the web. Financial data is used to help make predictions and spot patterns in the market that have occurred before. These patterns can be found using statistical analysis or running simulations. Textual information can be useful to discover consumer trends and opinions. One method to achieve this is through scraping HTML documents from microblog sites such as Twitter. Opinions and information can be extracted in quantities through Natural Language Processing (NLP). This is useful for brokers due to consumer opinion having influence over whether a share is seen as bullish or bearish in the market. Another reason behind the interest in both these fields is the speed at which computers can analyse data. On average, adults read 250-300 words per minute whereas a computer can download and analyse data at speed magnitudes faster. This processing power can provide real informational advantages when making decisions on investments.

One possible dataset that could be used by brokers to give an indication of stock performance is digital news articles. Major news organisations such as "The New York Times" and "The Wall Street Journal" release on average over 200 stories and videos a day. Most of this content contains well written factual information and presents knowledgeable opinions. While there has been research on using financial based news stories, there has not been many studies that have investigated the value of other categorised articles. This lack of knowledge inspires research and development into testing the following hypothesis:\\

\noindent\fbox{%
    \parbox{\textwidth}{%
        \textbf{Hypothesis:}\\
        The sentiment of news articles produced by mainstream news based on a given company correlates with the performance of that given company's share value.
    }%
}

\clearpage

To test this hypothesis, this experiment will be focusing on following the value of two highly valued companies, Apple and Facebook. This is due to them having a large media presence and having articles written about them on a regular basis. Alongside this, articles that are written about technology and other technology companies will be collected. This is to see if a  correlation can be found between the general sentiment of the technology sector and the markets value.

The experiment has two main phases, the first being performing sentiment analysis on newly scraped articles. The second is using statistical analysis to mark whether a given positive or negative sentiment correlates with a shares value. Alongside the experiment, a web platform has also been produced to display the predictions made by the sentiment analysis stage. For definitions of terms, you can reference the Nomenclature found in the appendix.

%-----------------------------------------------------
% Chapter: Ethical Considerations
%-----------------------------------------------------
\chapter{Ethical considerations}
\label{chap:ethical-con}

%-----------------------------------------------------
% Chapter: Background
%-----------------------------------------------------
\chapter{Background}
\label{chap:background}

In this section, the abstract problem of sentiment analysis will be discussed alongside existing studies that relate to this project.

\subsection{The abstract problem of sentiment anaylsis}
Before advancing, it is sensible to discuss what the task of sentiment analysis involves in further detail. Bing Lui describes in detail the abstract problem of sentiment analysis as:

\begin{displayquote}
Sentiment analysis, also called opinion mining, is the field of study that analyses peopleâ€™s opinions, sentiments, evaluations, appraisals, attitudes, and emotions towards entities such as products, services, organizations, individuals, issues, events, topics, and their attributes.
\end{displayquote}


To further this explanation, the definition of an opinion can be defined as the following quintuple:

\\\[Opinion (e_i, a_i_j, s_i_j_k_l, h_k, t_l)\]

In table \ref{table:sentiment}, an explanation has been provided to each component on how it relates to this experiment.

\begin{table}[h!]
\centering
\caption{Abstraction of an opinion}
\label{table:sentiment}
\begin{tabular}{|l|l|}
\hline
\[e_i\] & \begin{tabular}[c]{@{}l@{}}\(e_i\) is the entity that the news article is focusing on whether it is Apple, \\Facebook or the the subject of technology.\end{tabular} \\ \hline
\[a_i_j\] & \begin{tabular}[c]{@{}l@{}}\(a_i_j\) refers to the aspect of \(e_i\). Aspect refers to a particular part \\ or feature of the entity e.g. An article about the management \\ at the company Apple would translate to Apple being the entity \\ and the management being the aspect the article is referring to.\end{tabular} \\ \hline
\[s_i_j_k_l\] & \begin{tabular}[c]{@{}l@{}}\(s_i_j_k_l\) refers to the sentiment of this aspect, in the context of this \\ project, it will output a positive or negative sentiment.\end{tabular} \\ \hline
\[h_k\] & \begin{tabular}[c]{@{}l@{}}\(h_k\) is the opinion holder, in the context of this project, it is referring \\ to the news organisation that the article has been scraped the article from.\end{tabular} \\ \hline
\[t_l\] & \begin{tabular}[c]{@{}l@{}}\(t_l\) is the time frame of the opinion. Opinions can change rapidly, \\ especially in the stock market. It will be important to get the \\ latest opinions rather than scraping older opinions that are no longer \\ relevant.\end{tabular} \\ \hline
\end{tabular}
\end{table}

In the context of this experiment, sentiment analysis will be used to classify whether opinions found in digital articles are positive or negative towards Apple or Facebook. This classification will then be used to determine whether those companies share prices will rise or fall.

\subsection{Existing studies}

Examples of studies related to this project include research published by Bar-Haim and Feldman. Bar-Haim and Feldman attempted to identify expert investors and collect the content of their microblogs where expert knowledge would be shared. The aim of scraping these blogs was to see if better predictions of the value of a company could be made.  Until then, they had been using a noisy dataset which contained both poor and expert knowledge. This is similar to this projects experiment with the high-level idea of scraping high quality textual information in the hope of making accurate predictions of a company's value. The results concluded that using the expert opinions did cause a rise in accuracy in judging a companies value. More recently, there has been research papers published based on the same idea of using sentiment analysis on news articles. Written by Joshi Kalyani, H. N. Bharathi and Rao Jyothi, the basis of their experiment was collecting years of financial news articles focused on the company Apple. Using these articles they investigated the correlation between the mood towards Apple and the value of their stock. This is similar to what this study hopes to achieve. However,  more general news articles focused on Apple and Facebook will be scraped rather than focusing on financial news.

%-----------------------------------------------------
% Chapter: Requirement Analysis
%-----------------------------------------------------
\chapter{Requirement analysis}
\label{chap:requirement-anal}

The requirement analysis stage of a project is important. It presents clear achievable goals and reduces the risk of not delivering during implementation and the design of the project. Within this chapter, both functional and nonfunctional requirements will be defined for both the research stage and the application stage. The research stage is the implementation of the pipeline that would test the experiments hypothesis. The application stage takes the form of a web platform that was created to illustrate the pipelines predictions and results.

\section{Functional requirements}

Functional requirements refer to what the product should be able to do and what functionality it should provide

\subsection {Research requirements}
\begin{itemize}
  \item \textbf{Implement methods to scrape HTML files found on the internet}. The base of the experiment requires digital articles to be downloaded from their sources.
  \item \textbf{Implement methods to parse through each HTML page that has been downloaded and extract their textual data}. The value of these HTML files is the articles within them. It is important to only extract the useful data for analysis rather than everything found in the file.
    \item \textbf{Preprocess each parsed HTML file into a readable format for a machine to understand.} When performing NLP tasks, it is normal to preprocess textual information to standardise the data and also turn it into a format that can be analysed by algorithms.
    \item \textbf{Implement a topic classifier to classify whether an article is talking about Apple / Facebook or the Technology Sector}. It is essential that we classify newly scraped articles into a category so we can accurately track the behaviour of these companies shares.
    \item \textbf{Implement multiple machine learning models to extract a sentiment from a preprocessed article}. These models should take as input preprocessed news articles and return the sentiment of an article (positive or negative).
    \item \textbf{Automate analysis of the sentiments collected from the HTML documents when compared against the value of shares}. The analysis needs to be done on an automated scale due to the volume of articles that will be processed every day.
\end{itemize}

\subsection {Application requirements}
\begin{itemize}
  \item \textbf{Create a platform which displays the share behaviour predictions created by the research stage}. This is the main function of the application and will be used to demonstrate the progress and results of my experiment.
    \item \textbf{Create a platform which illustrates the behaviour of the Apple, Facebook and Technology share prices}. This will provide a graphical representation of how these companies shares are behaving and will ease in the understanding of the context of each prediction.
    \item \textbf{Create a platform that allows users to test each machine learning model developed in the research stage in classifying sentiment}. This is another important feature of the application and will be used to demonstrate how each machine learning model behaves.
\end{itemize}

\section{Nonfunctional requirements}
Nonfunctional requirements refer to how a system should behave and how the functional requirements should be implemented.

\subsection{Research requirements}
\begin{itemize}
    \item \textbf{Scrape HTML page at a regular time interval}. This is required due to the results of this experiment being time sensitive and requiring any new articles not to be missed.
  \item \textbf{Should be able to handle a lot of articles at once}. Alongside scraping at regular time intervals, it is important that the system can handle a large number of articles at a time as there may be in an influx of news released at one point in time.
    \item \textbf{Have new articles scraped and analysed at a fast speed}. As mentioned, the results of these experiments are time sensitive so it is important to have these analysed at a fast speed to improve accuracy when pinpointing a change in share behaviour.
    \item \textbf{Track the behaviour of Facebook / Apple / Technology value of a series of time intervals to be able to track changes across it all}. Part of this experiment is seeing how over a series of different time scales that news articles may be affecting the price of shares so it's important to implement this to get good, useful results.
\end{itemize}

\subsection{Application requirements}
\begin{itemize}
    \item \textbf{The platform should be responsive in design}. This is not essential but is an expected behaviour of websites created in the present day.
    \item \textbf{The platform should have an interface which can be easily understood and used by users}. When the platform is used, it should not create confusion and avoid frustration.
\end{itemize}

%-----------------------------------------------------
% Chapter: Project Plan
%-----------------------------------------------------
\chapter{Project Plan}
As mentioned previously, this project has two distinct parts. A research section where the hypothesis given within the introduction will be tested and an application section where a web platform will be created to display the results of the experiment. Due to these two parts contrasting, it is important to discuss both separately. At the end of this chapter, a section has been included which explains practices that will be followed in both parts.

\section{Research}
The research stage was the core of the dissertation and therefore it was sensible to predict that was going to take the majority of the time. After research, planning and a series of discussions with Bill, a blueprint of stages was created which would be followed. Fig blah blah shows the pipeline that was to be created and how the event-driven system was to be run including its triggers.

By breaking up this section into a series of components, it allowed better planning and time management. When meeting weekly with Bill clear deliverables were organised for the next meeting for each section.

\section{Application}
The application stage came second in priority for this project. This was due to the website content and requirements heavily dependent on the experiment to be started. Just like the research stage, however, it was split into different sections. The main tasks included:

\begin{itemize}
    \item{Create web server and environment}
    \item{Create backend processing to get prediction information}
    \item{Create backend system to test machine learning algorithms created in research stage}
    \item{Create backbone of web platform including pages and navigation}
    \item{Create tables and charts using HTML and Javascript}
    \item{Style each page on site}
    \item{Add textual content to each page}
\end{itemize}


%-----------------------------------------------------
% Chapter: Implementation
%-----------------------------------------------------
\chapter{Implementation}


\label{chap:implementation}

\subsection{Web Scraping}

\subsection{GDELT}

\subsection{Classifier API}

\subsection{Models}
\subsubsection{K Nearest Neigbors}
\subsubsection{Support Vector Machine}
\subsubsection{Linear Perceptron}
\subsubsection{Extra Trees Classifier}
\subsubsection{Naive Bayes Classifier}

\section{Website}
%-----------------------------------------------------
% Chapter: Results
%-----------------------------------------------------
\chapter{Results}
\label{chap:results}

%-----------------------------------------------------
% Chapter: Evaluation
%-----------------------------------------------------
\chapter{Evaluation}
\label{chap:evaluation}
\section{Possible Improvements}

%-----------------------------------------------------
% Chapter: Conclusion
%-----------------------------------------------------
\chapter{Conclusion}
\label{chap:conclusion}

\subsection{Further work}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% BIBLIOGRAPHY
\clearpage
\phantomsection
\addcontentsline{toc}{chapter}{Bibliography}
\bibliography{bib}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% START APPENDICES
\appendix
%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%-----------------------------------------------------
% Appendix: Code
%-----------------------------------------------------
\chapter{Code}
\label{app:code}

\begin{verbatim}
10 PRINT "HELLO WORLD"
\end{verbatim}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% END DOCUMENT
\end{document}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%
